{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:16:21.393540Z","iopub.execute_input":"2024-11-17T20:16:21.394133Z","iopub.status.idle":"2024-11-17T20:16:21.401992Z","shell.execute_reply.started":"2024-11-17T20:16:21.394084Z","shell.execute_reply":"2024-11-17T20:16:21.400728Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#把載入資料集改成roberta的格式","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset \n# 載入 Financial PhraseBank 資料集\ndataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:16:21.404028Z","iopub.execute_input":"2024-11-17T20:16:21.404394Z","iopub.status.idle":"2024-11-17T20:16:25.155328Z","shell.execute_reply.started":"2024-11-17T20:16:21.404356Z","shell.execute_reply":"2024-11-17T20:16:25.153745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification\n\n# 載入 RoBERTa 預訓練模型與分詞器\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)  # 3 個標籤\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:16:25.156810Z","iopub.execute_input":"2024-11-17T20:16:25.157195Z","iopub.status.idle":"2024-11-17T20:16:26.416400Z","shell.execute_reply.started":"2024-11-17T20:16:25.157156Z","shell.execute_reply":"2024-11-17T20:16:26.415376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 將數據分割為 80% 訓練集和 20% 測試集\ntrain_test_data = dataset[\"train\"].train_test_split(test_size=0.2)\n\n# 分別查看 train 和 test 的數據\nprint(train_test_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:16:26.417953Z","iopub.execute_input":"2024-11-17T20:16:26.418278Z","iopub.status.idle":"2024-11-17T20:16:26.454853Z","shell.execute_reply.started":"2024-11-17T20:16:26.418243Z","shell.execute_reply":"2024-11-17T20:16:26.453630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:20:02.713486Z","iopub.execute_input":"2024-11-17T20:20:02.714473Z","iopub.status.idle":"2024-11-17T20:20:02.719989Z","shell.execute_reply.started":"2024-11-17T20:20:02.714427Z","shell.execute_reply":"2024-11-17T20:20:02.718765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# 分詞函數\ndef tokenize_function(example):\n    return tokenizer(example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n\n# 對數據進行分詞\nencoded_dataset = train_test_data.map(tokenize_function, batched=True)\n\n# 設置數據格式\nencoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")  # 轉換標籤欄位名稱\nencoded_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:21:05.763803Z","iopub.execute_input":"2024-11-17T20:21:05.764304Z","iopub.status.idle":"2024-11-17T20:21:12.608292Z","shell.execute_reply.started":"2024-11-17T20:21:05.764261Z","shell.execute_reply":"2024-11-17T20:21:12.607153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 設定訓練參數 fine-tuning","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# 設置訓練參數\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",              # 模型輸出的目錄\n    evaluation_strategy=\"epoch\",        # 每個 epoch 評估一次\n    learning_rate=2e-5,                 # 學習率\n    per_device_train_batch_size=16,     # 每個設備的 batch size\n    per_device_eval_batch_size=16,      # 驗證 batch size\n    num_train_epochs=3,                 # 訓練的 epoch 數\n    weight_decay=0.01,                  # 權重衰減\n    save_strategy=\"epoch\",              # 每個 epoch 保存模型\n    logging_dir=\"./logs\",               # 日誌保存路徑\n    logging_steps=10,                   # 日誌輸出頻率\n    report_to=[],  # 禁用 W&B\n)\n\ntrainer = Trainer(\n    model=model,                         # 微調的模型\n    args=training_args,                  # 訓練參數\n    train_dataset=encoded_dataset[\"train\"],  # 訓練集\n    eval_dataset=encoded_dataset[\"test\"],    # 測試集\n)\n\n# 開始訓練\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:23:46.223197Z","iopub.execute_input":"2024-11-17T20:23:46.223624Z","iopub.status.idle":"2024-11-17T20:27:50.971767Z","shell.execute_reply.started":"2024-11-17T20:23:46.223582Z","shell.execute_reply":"2024-11-17T20:27:50.969998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./finetuned_roberta\")\ntokenizer.save_pretrained(\"./finetuned_roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:23:44.171905Z","iopub.status.idle":"2024-11-17T20:23:44.172370Z","shell.execute_reply.started":"2024-11-17T20:23:44.172138Z","shell.execute_reply":"2024-11-17T20:23:44.172162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import RobertaForSequenceClassification, RobertaTokenizer\n\n# 載入保存的微調模型\nmodel = RobertaForSequenceClassification.from_pretrained(\"./finetuned_roberta\")\ntokenizer = RobertaTokenizer.from_pretrained(\"./finetuned_roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T20:23:44.174188Z","iopub.status.idle":"2024-11-17T20:23:44.174603Z","shell.execute_reply.started":"2024-11-17T20:23:44.174395Z","shell.execute_reply":"2024-11-17T20:23:44.174416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 測試分類","metadata":{}},{"cell_type":"code","source":"# Sentiment test data\ntest_data = [\n    # Positive\n    \"The company's recent financial report exceeded market expectations.\",\n    \"Investors are confident in this new technology, leading to a significant stock price increase.\",\n    \"The management's decisive actions have resulted in substantial profits.\",\n    \"This deal will significantly expand the company's market share.\",\n    \"Customer feedback on the product has been overwhelmingly positive, driving sales upward.\",\n\n    # Neutral\n    \"The company plans to expand into new markets next year, but details are not yet disclosed.\",\n    \"The stock market showed little movement today as investors remained cautious.\",\n    \"The board meeting proceeded as scheduled, discussing future growth strategies.\",\n    \"The management announced a change in CEO, with a calm market response.\",\n    \"The company reported revenue growth in the quarterly report but fell short of its target.\",\n\n    # Negative\n    \"The product's market performance fell below expectations, leading to a decline in quarterly earnings.\",\n    \"Investors were disappointed with this policy, causing the stock price to drop.\",\n    \"The company is embroiled in a legal dispute and may face significant penalties.\",\n    \"A competitor's new product has created substantial pressure on the company.\",\n    \"Customer complaints about poor after-sales service have damaged the brand's reputation.\"\n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef classify_text(texts, model, tokenizer):\n    \"\"\"\n    使用微調的 RoBERTa 模型對文本進行分類。\n    :param texts: 一個包含待分類文本的列表\n    :param model: 微調的 RoBERTa 模型\n    :param tokenizer: 與模型匹配的 Tokenizer\n    :return: 每個文本的分類結果列表\n    \"\"\"\n    model.eval()  # 切換到評估模式\n    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n    \n    return predictions.cpu().numpy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 分類測試數據\npredictions = classify_text(test_data, model, tokenizer)\n\n# 將分類結果對應到實際標籤名稱\nlabel_mapping = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}  # 假設模型有這三個標籤\nresults = [label_mapping[pred] for pred in predictions]\n\n# 輸出結果\nfor text, sentiment in zip(test_data, results):\n    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}